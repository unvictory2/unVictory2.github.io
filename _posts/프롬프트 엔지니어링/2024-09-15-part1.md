---
layout : single
title : "PART 1 - 프롬프트 엔지니어링의 이론적 패경"
excerpt : "프롬프트 엔지니어링 - 챗GPT, 바드, 빙, 하이퍼클로바X까지 한 권으로 끝내기"
published: true
toc : true
toc_sticky : true

categories : 
    - Prompt Engineering

date : 2024-09-15
last modified : 2024-09-17
---

## 📝CHAPTER 1 - 프롬프트 엔지니어링은 질문을 잘하는 것이 아니다 ##

### 프롬프트의 정의
 - 프롬프트의 본래 정의 : 컴퓨터가 명령을 받을 준비가 됐을 때 사용자에게 띄우는 문구  
 - 업계의 통설적 정의 : 사람이 AI에게 제공하는 입력 문구

프롬프트 엔지니어링은 질문을 잘하는 법이라고 알려져 있지만, 본래 프롬프트의 정의를 적용해보면 **좋은 질문**을 하는 법이 아니라, **좋은 답변**을 얻는 방법이라고 볼 수 있다.

### 목표
어느 쪽 정의든 결과적으로 다음 프로세스를 따른다는 점에서는 완전히 같다.  
1. AI에 제공할 명령어를 설계
2. 이를 토대로 AI로부터 더 유용한 반응(답변)을 유도  

업계의 통설적 정의에 따르면 "그 이후 더 좋은 방법론을 탐구하고 체계적으로 정리"하는 과정이 필요하나 본래 정의에서는 필요하지 않음. 본래 정의에서는 필요한 만큼의 효용성만 달성되면 충분하다고 보기 때문. 앞으로의 내용에선 2번 과정에 집중할 예정이다.

<br>

## 📝CHAPTER 2 - 모든 것은 어텐션으로부터 시작되었다  

### 어텐션 (Attention)
 - 챗GPT, 바드, 라마, 람다, 코파일럿 등의 AI들은 전부 구글이 개발한 <u>트랜스포머</u>라는 AI 기술을 이리저리 개조해서 만들어진 AI
 - 이 트랜스포머는 <u>어텐션</u>이라는 기술을 바탕으로 만들어짐
 - 즉 현대의 초거대 AI는 태생적으로 어텐션의 특징을 계승

참고로 이건 2023년 10월 책이라 그런지 바드가 시연회에서 할루시네이션을 보였다는 건 써있으나 Gemini에 대한 건 써있지 않다.

### 원리와 쓸모  
어텐션을 수학적으로 설명하면 매우 어려우나, 우리가 영어 독해를 하는 과정에 비유하면 쉽다 : 중요해 보이는 부분에 동그라미를 치고, 뒷부분을 읽고 와서 앞부분과 이어 붙이고 등의 과정이 어텐션이다. (중요해 보이는 내용을 표시하고 요약하는 과정을 비유한듯) 

그럼 어텐션은 왜 쓸모 있는 걸까? AI가 대량의 텍스트를 읽고 이해하는데 큰 도움이 되기 때문이다. 한 페이지 분량의 내용을 읽고 기억할 수 있는 AI가 있다고 가정하자. 이 AI에게 몇십권 분량의 글을 모두 읽고 기억하게 한다면 정상적으로 동작하지 않을 거다. 하지만 책을 처음부터 끝까지 모두 읽기 보다는, <u>가볍게 훑어보며 중요해보이는 부분에만 표시하며 내용을 압축</u>하게 하면 어떨까? 나중에 이 정보가 필요할 때 미리 압축해둔 내용을 빠르게 읽고 올 수 있다. 이게 바로 어텐션의 역할이다.   

AI는 내가 제공한 정보뿐만 아니라 검색 결과로 나온 정보에 대해서도 어텐션을 실시한다. 또 챗GPT의 경우 대답을 할 때 어텐션을 이용해 지금까지의 대화를 빠르게 훑고 온다. 이렇듯 AI의 뛰어난 성능의 기반에는 어텐션이 있다.  

<br>

## 📝CHAPTER 3 - 당신은 LLM과 그 사용법을 오해하고 있다

### 할루시네이션은 잘못된 걸까?
 - 할루시네이션 : AI가 잘못된 정보를 마치 진실처럼 전달하는 현상
 - 챗GPT나 바드 같은 AI를 제작하는 과정에서 지식을 체계적으로 주입하고 암기시키는 과정은 존재하지 않는다.
 - LLM은 인간이 언어를 사용하는 방식에 대해 이해하고 학습한 AI지, 지식을 정확하게 전달하기 위해 만들어진 AI가 아니다.

### AI의 지식 저장 방식
 - 손실 압축 : 인간의 뇌가 정보를 저장하는 방법. 전체 정보 중 불필요한 부분은 버리고, 중요한 부분은 남기는 과정. 예를 들어 어제 먹은 수박의 줄무늬나 씨앗의 개수는 잊지만, 어떤 상황에서 먹었는지에 대한 관념은 기억한다.
 - 인코딩 : AI분야에서 손실 압축에 해당되는 개념. 담당하는 구조물은 인코더. 외부의 정보를 AI에 입력하는 과정을 의미하는데, AI의 추상화 능력과 이해력과 연관된다.
 - 디코딩 : 압축된 정보를 끄집어내어 표현하는 과정. AI의 작문 솜씨나 그림 솜씨와 연관돼있음. (표현력)

### AI의 지식 저장 형태

가상 실험을 해보자.
1. 딱 3개의 뉴런을 가지고 있는 가상의 벌레를 상상해보자.
2. 뉴런1은 단맛, 뉴런2는 신맛, 뉴런3은 아삭아삭한 식감에 반응한다.
3. 벌레는 사과, 수박, 딸기를 차례대로 먹는다. 반응한 뉴런은 1, 반응하지 않은 뉴런은 0이라고 부른다.
4. - 사과를 먹으면 시고 아삭 -> `(0,1,1)`
   - 수박을 먹으면 달고 아삭 -> `(1,0,1)`
   - 딸기를 먹으면 새콤달콤 -> `(1,1,0)`
   - 숫자 3개의 묶음으로 각 과일에 대한 정보를 손실 압축해냈다.

- 이 묶음을 각자 x,y,z축 좌표값으로 보면 <u>각 정보는 3차원 공간의 벡터값</u>이고, 레이턴트 벡터(latent vector)라고 부른다. 벡터가 놓이는 공간은 레이턴트 스페이스(latent space)라고 부른다. 인간의 뇌는 뉴런이 훨씬 더 많기 때문에 이 공간이 더 크다.
- 비슷한 정보는 레이턴트 스페이스에서도 가까운 곳에 있고, 그 반대도 마찬가지다.
- 디코더는 레이턴트 벡터의 정보를 우리가 이해할 수 있는 데이터로 팽창한다. 숫자들의 조합에서 그림을 그려내거나, 설명문을 작성하는 등.

이를 응용한 추론 기능도 알아보자.

1. 벌레에게 모르는 음식을 먹였는데 반응이 `(1,0,1)`이다.
2. `(1,0,1)`이면 수박이라고 추론할 수 있다. -> 새 정보가 레이턴트 스페이스의 어디에 위치하는 벡터인지를 토대로 정보를 추론할 수도 있다.
3. 하지만 치킨을 먹었어도 `(1,0,1)`이라는 신호가 떴을 거다. 즉 벌레는 치킨과 수박을 구분할 수 없다.
4. 매콤함이나 느끼함 등을 구분하는 뉴런이 없기 때문에 구분하지 못한다. ( = 뇌세포의 개수가 적다, 레이턴트 스페이스의 부피가 너무 작다)

이에 대한 해결 방안은?
- OpenAI : 뇌세포가 많으면 AI의 성능이 올라갈 거다. 
- 메타 : 부피도 중요하지만 설계 자체를 세밀하게 하는 게 더 중요하다. 

### AI 산업의 역사
1. 2013 : 구글의 `Word2Vec`
   - 빈칸 맞추기 퀴즈와 어울리는 단어 찾기 퀴즈 두 개로 단어의 의미 학습. 
   - 어떤 빈칸에 어떤 단어가 오면 좋겠다던가, 비슷한 구조의 문장에서 자주 등장하는 유사한 의미의 단어에 대한 정보 등을 레이턴트 스페이스에 벡터 형태로 표시.
   - 문장을 입력 받음 > 인코더로 손실 압축 > 관심 단어의 의미만 벡터로 압축  
  
2. 2014 : 구글의 `Seq2Seq`  
   - 단어를 넘어 문장의 의미를 통째로 이해 가능.
   - 인코더-디코더 구조 : 문장 입력 > 인코더 손실 압축 > 벡터화 > 디코더 압축 해제 > 문장 출력
   - 이 구조는 <u>현대의 LLM까지 그대로 계승</u>된 철학이다.

3. 어텐션
   - 용량의 한계가 있었으나 위에서 설명한 어텐션의 등장으로 해결했다.
   - 인코더나 디코더에 어텐션을 사용해 획기적으로 좋은 결과물을 얻어냈다.
  
4. 2017 : 구글의 `Transformer` (GPT의 T)
   - '그냥 인코더와 디코더의 개수를 늘리고, 전부 어텐션을 붙이면 이해력과 표현력이 좋아지지 않을까?'라는 발상을 구현한 AI.
   - Seq2Seq에 비해서 경이로운 성능을 보여줌, 현대 LLM들이 죄다 트랜스포머를 사용하는 게 그 반증.
  
5. 2018 : OpenAI의 `GPT`와 구글의 `BERT`
   - GPT는 디코더(표현력)만 쌓아놓은 AI, BERT는 인코더(이해력)만 쌓아놓은 AI.
   - BERT의 성능이 압도적이었다.
  
6. 2020 : OpenAI의 `GPT-3`
   - BERT의 논문에는 "AI의 부피를 두 배 키웠더니 성능은 5%밖에 증가하지 않더라"는 말이 있었기에, AI의 부피보다는 설계를 잘 하는 게 중요하다고 생각하는 게 합리적이다.
   - 그러나 OpenAI는 부피를 늘리면 성능이 증가하기는 한다는 걸 보고 BERT보다 583배 큰 GPT-3를 만들었다.
   - 성능은 고작 몇 퍼센트 차이지만, 이를 기반으로 만들어진 챗GPT의 파급력을 보면 효율은 별로여도 무시할만한 차이는 아닌듯하다.
   - 이런 AI를 초거대 AI라고 부르며 LLM 역시 초거대 AI의 일종이다. 

### Pre-Training이란?
- GPT는 `Generative Pre-trained Transformer`의 약자
- `Generative`는 뭔가를 생성한다는 뜻이고, `Transformer`는 위에서 나온 구글의 AI
- `Pre-trained`란 어떤 분야의 매우 깊은 경험을 가진 사람을 데려다가 비슷하지만 약간은 다른 임무에 투입하는 것. 책에선 소바 삶기와 스파게티 삶기가 예시로 나온다.
- GPT나 웬만한 다른 LLM들은, 언어와는 관련이 있지만 채팅과는 무관한 임무에서 경력을 쌓고 채팅에서 활용된다.

그럼 뭘로 경력을 쌓는 걸까? 2013년부터 하던 단어 퀴즈와 크게 다르지 않다.  
문장의 일부를 트랜츠포머에게 보여주고 다음에 올 단어로 올바른 단어를 맞추게 한다. 이 과정에서도 어텐션이 작동한다.
- 원본 문장 : 강아지는 식탐이 많은 동물이다.
- 1단계 : 강아지는 ??? 인 상황 : GPT가 그 뒤에 오는 단어는 `식탐이`라고 예측
- 2단계 : 강아지는 식탐이 ??? 인 상황 : `많은`이 와야 한다고 예측
- 3단계 : 강아지는 식탐이 많은 ??? 인 상황 : `동물이다`가 와야 한다고 예측

이 과정에서 인간이 단어를 나열하는 순서에 숨겨진 문법 구조와 논리 전개 순서를 학습한다. 이런 단어 퀴즈를 1000억 원이 넘는 슈퍼컴퓨터로 매월 전기세만 50억 이상 지출하면서 1년 내내 단어 퀴즈를 풀고 있는 거다.  

그 결과 <u>다음에 올 단어를 기가 막히게 잘 맞히는 AI</u>가 만들어지고 이게 채팅을 하면 챗GPT가 된다.  
<u>AI가 지식을 따로 암기하거나 공부하는 과정은 없으며,</u> 방대한 텍스트를 학습하다 보니 덩달아 학습하게 된 것 뿐이다. 그 흔적은 희석되어 레이턴트 스페이스 위에 남아 있어 우리의 질문에 그럴싸한 답을 생성해낸다.  

즉 LLM의 본질은 : 
- 척척박사 아님
- 인간의 언어를 굉장히 잘 이해한 AI
- 어텐션에 의존하는 특징이 있어 한계도 있음

프롬프트 엔지니어링은 이를 이해하고 어텐션과 관련된 특징을 활용하여 나은 결과를 만들어내는 AI 활용 방법이다.