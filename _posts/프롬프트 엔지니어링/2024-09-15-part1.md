---
layout : single
title : "PART 1 - 프롬프트 엔지니어링의 이론적 패경"
excerpt : "프롬프트 엔지니어링 - 챗GPT, 바드, 빙, 하이퍼클로바X까지 한 권으로 끝내기"
published: true
toc : true
toc_sticky : true

categories : 
    - Prompt Engineering

date : 2024-09-15
last modified : 2024-09-16
---

# PART 1 - 프롬프트 엔지니어링의 이론적 배경 #

<br>

## 📝CHAPTER 1 - 프롬프트 엔지니어링은 질문을 잘하는 것이 아니다 ##

### 프롬프트의 정의
 - 프롬프트의 본래 정의 : 컴퓨터가 명령을 받을 준비가 됐을 때 사용자에게 띄우는 문구  
 - 업계의 통설적 정의 : 사람이 AI에게 제공하는 입력 문구

프롬프트 엔지니어링은 질문을 잘하는 법이라고 알려져 있지만, 본래 프롬프트의 정의를 적용해보면 **좋은 질문**을 하는 법이 아니라, **좋은 답변**을 얻는 방법이라고 볼 수 있다.

### 목표
어느 쪽 정의든 결과적으로 다음 프로세스를 따른다는 점에서는 완전히 같다.  
1. AI에 제공할 명령어를 설계
2. 이를 토대로 AI로부터 더 유용한 반응(답변)을 유도  

업계의 통설적 정의에 따르면 "그 이후 더 좋은 방법론을 탐구하고 체계적으로 정리"하는 과정이 필요하나 본래 정의에서는 필요하지 않음. 본래 정의에서는 필요한 만큼의 효용성만 달성되면 충분하다고 보기 때문. 앞으로의 내용에선 2번 과정에 집중할 예정이다.

<br>

## 📝CHAPTER 2 - 모든 것은 어텐션으로부터 시작되었다  

### 어텐션 (Attention)
 - 챗GPT, 바드, 라마, 람다, 코파일럿 등의 AI들은 전부 구글이 개발한 <u>트랜스포머</u>라는 AI 기술을 이리저리 개조해서 만들어진 AI
 - 이 트랜스포머는 <u>어텐션</u>이라는 기술을 바탕으로 만들어짐
 - 즉 현대의 초거대 AI는 태생적으로 어텐션의 특징을 계승

참고로 이건 2023년 10월 책이라 그런지 바드가 시연회에서 할루시네이션을 보였다는 건 써있으나 Gemini에 대한 건 써있지 않다.

### 원리와 쓸모  
어텐션을 수학적으로 설명하면 매우 어려우나, 우리가 영어 독해를 하는 과정에 비유하면 쉽다 : 중요해 보이는 부분에 동그라미를 치고, 뒷부분을 읽고 와서 앞부분과 이어 붙이고 등의 과정이 어텐션이다. (중요해 보이는 내용을 표시하고 요약하는 과정을 비유한듯) 

그럼 어텐션은 왜 쓸모 있는 걸까? AI가 대량의 텍스트를 읽고 이해하는데 큰 도움이 되기 때문이다. 한 페이지 분량의 내용을 읽고 기억할 수 있는 AI가 있다고 가정하자. 이 AI에게 몇십권 분량의 글을 모두 읽고 기억하게 한다면 정상적으로 동작하지 않을 거다. 하지만 책을 처음부터 끝까지 모두 읽기 보다는, <u>가볍게 훑어보며 중요해보이는 부분에만 표시하며 내용을 압축</u>하게 하면 어떨까? 나중에 이 정보가 필요할 때 미리 압축해둔 내용을 빠르게 읽고 올 수 있다. 이게 바로 어텐션의 역할이다.   

AI는 내가 제공한 정보뿐만 아니라 검색 결과로 나온 정보에 대해서도 어텐션을 실시한다. 또 챗GPT의 경우 대답을 할 때 어텐션을 이용해 지금까지의 대화를 빠르게 훑고 온다. 이렇듯 AI의 뛰어난 성능의 기반에는 어텐션이 있다.  

<br>

## 📝CHAPTER 3 - 당신은 LLM과 그 사용법을 오해하고 있다

### 할루시네이션은 잘못된 걸까?
 - 할루시네이션 : AI가 잘못된 정보를 마치 진실처럼 전달하는 현상
 - 챗GPT나 바드 같은 AI를 제작하는 과정에서 지식을 체계적으로 주입하고 암기시키는 과정은 존재하지 않는다.
 - LLM은 인간이 언어를 사용하는 방식에 대해 이해하고 학습한 AI지, 지식을 정확하게 전달하기 위해 만들어진 AI가 아니다.

### AI의 지식 저장 방식
 - 손실 압축 : 인간의 뇌가 정보를 저장하는 방법. 전체 정보 중 불필요한 부분은 버리고, 중요한 부분은 남기는 과정. 예를 들어 어제 먹은 수박의 줄무늬나 씨앗의 개수는 잊지만, 어떤 상황에서 먹었는지에 대한 관념은 기억한다.
 - 인코딩 : AI분야에서 손실 압축에 해당되는 개념. 담당하는 구조물은 인코더. 외부의 정보를 AI에 입력하는 과정을 의미하는데, AI의 추상화 능력과 이해력과 연관된다.
 - 디코딩 : 압축된 정보를 끄집어내어 표현하는 과정. AI의 작문 솜씨나 그림 솜씨와 연관돼있음. 

### AI의 지식 저장 형태

가상 실험을 해보자.
1. 딱 3개의 뉴런을 가지고 있는 가상의 벌레를 상상해보자.
2. 뉴런1은 단맛, 뉴런2는 신맛, 뉴런3은 아삭아삭한 식감에 반응한다.
3. 벌레는 사과, 수박, 딸기를 차례대로 먹는다. 반응한 뉴런은 1, 반응하지 않은 뉴런은 0이라고 부른다.
4. - 사과를 먹으면 시고 아삭 -> (0,1,1)
   - 수박을 먹으면 달고 아삭 -> (1,0,1)
   - 딸기를 먹으면 새콤달콤 -> (1,1,0)
   - 숫자 3개의 묶음으로 각 과일에 대한 정보를 손실 압축해냈다.

- 이 묶음을 각자 x,y,z축 좌표값으로 보면 각 정보는 3차원 공간의 벡터값이고, 레이턴트 벡터(latent vector)라고 부른다. 벡터가 놓이는 공간은 레이턴트 스페이스(latent space)라고 부른다. 인간의 뇌는 뉴런이 훨씬 더 많기 때문에 이 공간이 더 크다.
- 비슷한 정보는 레이턴트 스페이스에서도 가까운 곳에 있고, 그 반대도 마찬가지다.
- 디코더는 레이턴트 벡터의 정보를 우리가 이해할 수 있는 데이터로 팽창한다. 숫자들의 조합에서 그림을 그려내거나, 설명문을 작성하는 등.

추론 기능도 알아보자.

1. 벌레에게 모르는 음식을 먹였는데 반응이 (1,0,1)이다.
2. (1,0,1)이면 수박이라고 추론할 수 있다. -> 새 정보가 레이턴트 스페이스의 어디에 위치하는 벡터인지를 토대로 정보를 추론할 수도 있다.
3. 하지만 치킨을 먹었어도 (1,0,1)이라는 신호가 떴을 거다. 즉 벌레는 치킨과 수박을 구분할 수 없다.
4. 매콤함이나 느끼함 등을 구분하는 뉴런이 없기 때문에 구분하지 못한다. ( = 뇌세포의 개수가 적다, 레이턴트 스페이스의 부피가 너무 작다)

이에 대한 기업들의 생각은?
- OpenAI : 뇌세포가 많으면 AI의 성능이 올라갈 거다. 
- 메타 : 부피도 중요하지만 설계 자체를 세밀하게 하는 게 더 중요하다. 
